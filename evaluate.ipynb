{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06712baf-bef6-4de3-b5e0-1cc5059b76c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoaders created successfully!\n",
      "\n",
      "=== Evaluation Results ===\n",
      "\n",
      "Model: checkpoints/best_model_fold1.pth\n",
      "\n",
      "Overall Accuracy: 0.9966\n",
      "Balanced Accuracy: 0.9966\n",
      "ROC AUC: 0.9999\n",
      "PR AUC: 1.0000\n",
      "Matthew's CC: 0.9932\n",
      "Cohen's Kappa: 0.9932\n",
      "\n",
      "Clinical Metrics:\n",
      "Sensitivity/Recall: 0.9974\n",
      "Specificity: 0.9957\n",
      "PPV: 0.9974\n",
      "NPV: 0.9957\n",
      "Positive LR: 233.40\n",
      "Negative LR: 0.00\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       1.00      1.00      1.00       234\n",
      "   Pneumonia       1.00      1.00      1.00       390\n",
      "\n",
      "    accuracy                           1.00       624\n",
      "   macro avg       1.00      1.00      1.00       624\n",
      "weighted avg       1.00      1.00      1.00       624\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[233, 1], [1, 389]]\n",
      "\n",
      "Results saved to evaluation_results.json\n",
      "\n",
      "Evaluation complete. Plots saved to 'plots/' directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14932/1441998566.py:129: FutureWarning: \n",
      "\n",
      "`shade` is now deprecated in favor of `fill`; setting `fill=True`.\n",
      "This will become an error in seaborn v0.14.0; please update your code.\n",
      "\n",
      "  sns.kdeplot(y_probs[y_true == i, 1], label=class_name, shade=True)\n",
      "/tmp/ipykernel_14932/1441998566.py:129: FutureWarning: \n",
      "\n",
      "`shade` is now deprecated in favor of `fill`; setting `fill=True`.\n",
      "This will become an error in seaborn v0.14.0; please update your code.\n",
      "\n",
      "  sns.kdeplot(y_probs[y_true == i, 1], label=class_name, shade=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                           roc_auc_score, average_precision_score,\n",
    "                           matthews_corrcoef, balanced_accuracy_score,\n",
    "                           cohen_kappa_score, roc_curve, auc)\n",
    "from model import get_model\n",
    "from data_prep import get_dataloaders, custom_collate\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    MODEL_CHECKPOINT = \"checkpoints/best_model_fold1.pth\"  # best model path\n",
    "    PLOTS_DIR = \"plots_test\"\n",
    "    BATCH_SIZE = 32\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Setup directories\n",
    "os.makedirs(Config.PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "def load_model(model_path: str) -> torch.nn.Module:\n",
    "    \"\"\"Load trained model from checkpoint\"\"\"\n",
    "    model = get_model()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=Config.DEVICE))\n",
    "    model.to(Config.DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model: torch.nn.Module, test_loader: DataLoader) -> dict:\n",
    "    \"\"\"Run evaluation on test set and return metrics\"\"\"\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(Config.DEVICE)\n",
    "            labels = labels.to(Config.DEVICE)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    \n",
    "    y_true = np.concatenate(all_labels)\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_probs = np.concatenate(all_probs)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_probs[:, 1])\n",
    "    pr_auc = average_precision_score(y_true, y_probs[:, 1])\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    # Confusion matrix metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = tp / (tp + fp)\n",
    "    npv = tn / (tn + fn)\n",
    "    plr = sensitivity / (1 - specificity)\n",
    "    nlr = (1 - sensitivity) / specificity\n",
    "    \n",
    "    return {\n",
    "        'y_true': y_true.tolist(),\n",
    "        'y_pred': y_pred.tolist(),\n",
    "        'y_probs': y_probs.tolist(),\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'mcc': mcc,\n",
    "        'kappa': kappa,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'ppv': ppv,\n",
    "        'npv': npv,\n",
    "        'plr': plr,\n",
    "        'nlr': nlr,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'classification_report': classification_report(y_true, y_pred, target_names=['Normal', 'Pneumonia'], output_dict=True)\n",
    "    }\n",
    "\n",
    "def generate_plots(metrics: dict):\n",
    "    \"\"\"Generate evaluation visualizations\"\"\"\n",
    "    y_true = np.array(metrics['y_true'])\n",
    "    y_probs = np.array(metrics['y_probs'])\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Pneumonia'],\n",
    "                yticklabels=['Normal', 'Pneumonia'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(f'{Config.PLOTS_DIR}/confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f'{Config.PLOTS_DIR}/roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Probability Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, class_name in enumerate(['Normal', 'Pneumonia']):\n",
    "        sns.kdeplot(y_probs[y_true == i, 1], label=class_name, shade=True)\n",
    "    plt.xlabel('Predicted Probability of Pneumonia')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Predicted Probability Distribution by True Class')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{Config.PLOTS_DIR}/probability_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "def print_results(metrics: dict):\n",
    "    \"\"\"Print formatted evaluation results\"\"\"\n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    print(f\"\\nModel: {Config.MODEL_CHECKPOINT}\")\n",
    "    print(f\"\\nOverall Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"PR AUC: {metrics['pr_auc']:.4f}\")\n",
    "    print(f\"Matthew's CC: {metrics['mcc']:.4f}\")\n",
    "    print(f\"Cohen's Kappa: {metrics['kappa']:.4f}\")\n",
    "    \n",
    "    print(\"\\nClinical Metrics:\")\n",
    "    print(f\"Sensitivity/Recall: {metrics['sensitivity']:.4f}\")\n",
    "    print(f\"Specificity: {metrics['specificity']:.4f}\")\n",
    "    print(f\"PPV: {metrics['ppv']:.4f}\")\n",
    "    print(f\"NPV: {metrics['npv']:.4f}\")\n",
    "    print(f\"Positive LR: {metrics['plr']:.2f}\")\n",
    "    print(f\"Negative LR: {metrics['nlr']:.2f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        metrics['y_true'],\n",
    "        metrics['y_pred'],\n",
    "        target_names=['Normal', 'Pneumonia']\n",
    "    ))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "\n",
    "def save_results(metrics: dict, filename: str = \"evaluation_results.json\"):\n",
    "    \"\"\"Save evaluation metrics to JSON file\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    print(f\"\\nResults saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    # Load data and model\n",
    "    _, _, test_loader = get_dataloaders()\n",
    "    model = load_model(Config.MODEL_CHECKPOINT)\n",
    "    \n",
    "    # Run evaluation\n",
    "    metrics = evaluate_model(model, test_loader)\n",
    "    \n",
    "    # Generate outputs\n",
    "    generate_plots(metrics)\n",
    "    print_results(metrics)\n",
    "    save_results(metrics)\n",
    "    \n",
    "    print(\"\\nEvaluation complete. Plots saved to 'plots/' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f56bea-421f-4ed8-a03d-3c4e4ab09101",
   "metadata": {},
   "source": [
    "# Evluate 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f762340-9972-46c5-800d-039a427c816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoaders created successfully!\n",
      "\n",
      "=== Evaluation Results ===\n",
      "\n",
      "Model: checkpoints/best_model_fold1.pth\n",
      "\n",
      "Performance Metrics:\n",
      "Accuracy: 0.9966\n",
      "ROC AUC: 0.9999\n",
      "PR AUC: 1.0000\n",
      "Matthew's CC: 0.9932\n",
      "Cohen's Kappa: 0.9932\n",
      "\n",
      "Clinical Metrics:\n",
      "Sensitivity/Recall: 0.9974\n",
      "Specificity: 0.9957\n",
      "PPV: 0.9974\n",
      "NPV: 0.9957\n",
      "Positive LR: 233.40\n",
      "Negative LR: 0.00\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       1.00      1.00      1.00       234\n",
      "   Pneumonia       1.00      1.00      1.00       390\n",
      "\n",
      "    accuracy                           1.00       624\n",
      "   macro avg       1.00      1.00      1.00       624\n",
      "weighted avg       1.00      1.00      1.00       624\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[233   1]\n",
      " [  1 389]]\n",
      "\n",
      "Evaluation complete. Results saved to 'evaluation_plots/'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                           roc_auc_score, average_precision_score,\n",
    "                           matthews_corrcoef, balanced_accuracy_score,\n",
    "                           cohen_kappa_score, roc_curve, auc,\n",
    "                           precision_recall_curve)\n",
    "from model import get_model\n",
    "from data_prep import get_dataloaders, custom_collate\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model_path: str = \"checkpoints/best_model_fold1.pth\"):\n",
    "        self.config = {\n",
    "            \"model_path\": model_path,\n",
    "            \"plots_dir\": \"evaluation_plots\",\n",
    "            \"batch_size\": 32,\n",
    "            \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        }\n",
    "        os.makedirs(self.config[\"plots_dir\"], exist_ok=True)\n",
    "        \n",
    "    def load_model(self) -> torch.nn.Module:\n",
    "        model = get_model()\n",
    "        model.load_state_dict(torch.load(self.config[\"model_path\"], \n",
    "                                      map_location=self.config[\"device\"]))\n",
    "        model.to(self.config[\"device\"])\n",
    "        model.eval()\n",
    "        return model\n",
    "    \n",
    "    def evaluate(self) -> Dict:\n",
    "        model = self.load_model()\n",
    "        _, _, test_loader = get_dataloaders()\n",
    "        \n",
    "        results = self._get_predictions(model, test_loader)\n",
    "        metrics = self._calculate_metrics(results)\n",
    "        self._generate_visualizations(results, metrics)\n",
    "        self._save_results(metrics)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _get_predictions(self, model: torch.nn.Module, \n",
    "                       test_loader: DataLoader) -> Dict:\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        all_images = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(self.config[\"device\"])\n",
    "                outputs = model(images)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "                all_images.append(images.cpu().numpy())\n",
    "        \n",
    "        return {\n",
    "            \"y_true\": np.concatenate(all_labels),\n",
    "            \"y_pred\": np.concatenate(all_preds),\n",
    "            \"y_probs\": np.concatenate(all_probs),\n",
    "            \"images\": np.concatenate(all_images)\n",
    "        }\n",
    "    \n",
    "    def _calculate_metrics(self, results: Dict) -> Dict:\n",
    "        y_true = results[\"y_true\"]\n",
    "        y_pred = results[\"y_pred\"]\n",
    "        y_probs = results[\"y_probs\"]\n",
    "        \n",
    "        metrics = {\n",
    "            \"accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "            \"roc_auc\": roc_auc_score(y_true, y_probs[:, 1]),\n",
    "            \"pr_auc\": average_precision_score(y_true, y_probs[:, 1]),\n",
    "            \"mcc\": matthews_corrcoef(y_true, y_pred),\n",
    "            \"kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "            \"y_true\": y_true.tolist(),\n",
    "            \"y_pred\": y_pred.tolist()\n",
    "        }\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        metrics.update({\n",
    "            \"confusion_matrix\": cm.tolist(),\n",
    "            \"sensitivity\": tp / (tp + fn),\n",
    "            \"specificity\": tn / (tn + fp),\n",
    "            \"ppv\": tp / (tp + fp),\n",
    "            \"npv\": tn / (tn + fn),\n",
    "            \"plr\": (tp / (tp + fn)) / (1 - (tn / (tn + fp))),\n",
    "            \"nlr\": (1 - (tp / (tp + fn))) / (tn / (tn + fp)),\n",
    "            \"classification_report\": classification_report(\n",
    "                y_true, y_pred, \n",
    "                target_names=['Normal', 'Pneumonia'], \n",
    "                output_dict=True\n",
    "            )\n",
    "        })\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _generate_visualizations(self, results: Dict, metrics: Dict):\n",
    "        y_true = results[\"y_true\"]\n",
    "        y_pred = results[\"y_pred\"]\n",
    "        y_probs = results[\"y_probs\"]\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(metrics[\"confusion_matrix\"], \n",
    "                   annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Normal', 'Pneumonia'],\n",
    "                   yticklabels=['Normal', 'Pneumonia'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.savefig(os.path.join(self.config[\"plots_dir\"], 'confusion_matrix.png'), \n",
    "                   bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_probs[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC (AUC = {roc_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(os.path.join(self.config[\"plots_dir\"], 'roc_curve.png'), \n",
    "                   bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Precision-Recall Curve\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_probs[:, 1])\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(recall, precision, color='blue', lw=2, \n",
    "                label=f'PR (AUC = {metrics[\"pr_auc\"]:.4f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.savefig(os.path.join(self.config[\"plots_dir\"], 'pr_curve.png'), \n",
    "                   bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Probability Distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i, class_name in enumerate(['Normal', 'Pneumonia']):\n",
    "            sns.kdeplot(y_probs[y_true == i, 1], \n",
    "                       label=class_name, \n",
    "                       fill=True)\n",
    "        plt.xlabel('Predicted Probability of Pneumonia')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Predicted Probability Distribution by True Class')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(self.config[\"plots_dir\"], 'probability_distribution.png'), \n",
    "                   bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Misclassified Examples (if any)\n",
    "        if (y_true != y_pred).sum() > 0:\n",
    "            self._plot_misclassified(results)\n",
    "    \n",
    "    def _plot_misclassified(self, results: Dict):\n",
    "        incorrect = results[\"y_true\"] != results[\"y_pred\"]\n",
    "        num_samples = min(5, incorrect.sum())\n",
    "        indices = np.where(incorrect)[0][:num_samples]\n",
    "        \n",
    "        plt.figure(figsize=(15, 3 * num_samples))\n",
    "        for i, idx in enumerate(indices):\n",
    "            img = results[\"images\"][idx].transpose(1, 2, 0)\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "            \n",
    "            plt.subplot(num_samples, 1, i+1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(\n",
    "                f\"True: {'Normal' if results['y_true'][idx]==0 else 'Pneumonia'} | \"\n",
    "                f\"Pred: {'Normal' if results['y_pred'][idx]==0 else 'Pneumonia'} | \"\n",
    "                f\"Prob: {results['y_probs'][idx, 1]:.3f}\"\n",
    "            )\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.config[\"plots_dir\"], 'misclassified_examples.png'), \n",
    "                   bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def _save_results(self, metrics: Dict):\n",
    "        \"\"\"Save evaluation results to JSON and CSV\"\"\"\n",
    "        # Save full metrics to JSON\n",
    "        with open(os.path.join(self.config[\"plots_dir\"], 'evaluation_results.json'), 'w') as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "        \n",
    "        # Save key metrics to CSV\n",
    "        metrics_df = pd.DataFrame({\n",
    "            \"Metric\": [\n",
    "                \"Accuracy\", \"Balanced Accuracy\", \"ROC AUC\", \"PR AUC\",\n",
    "                \"Matthew's CC\", \"Cohen's Kappa\", \"Sensitivity\", \"Specificity\",\n",
    "                \"PPV\", \"NPV\", \"Positive LR\", \"Negative LR\"\n",
    "            ],\n",
    "            \"Value\": [\n",
    "                metrics[\"accuracy\"], metrics[\"accuracy\"],\n",
    "                metrics[\"roc_auc\"], metrics[\"pr_auc\"],\n",
    "                metrics[\"mcc\"], metrics[\"kappa\"],\n",
    "                metrics[\"sensitivity\"], metrics[\"specificity\"],\n",
    "                metrics[\"ppv\"], metrics[\"npv\"],\n",
    "                metrics[\"plr\"], metrics[\"nlr\"]\n",
    "            ]\n",
    "        })\n",
    "        metrics_df.to_csv(os.path.join(self.config[\"plots_dir\"], 'key_metrics.csv'), index=False)\n",
    "    \n",
    "    def print_results(self, metrics: Dict):\n",
    "        print(\"\\n=== Evaluation Results ===\")\n",
    "        print(f\"\\nModel: {self.config['model_path']}\")\n",
    "        \n",
    "        print(\"\\nPerformance Metrics:\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "        print(f\"PR AUC: {metrics['pr_auc']:.4f}\")\n",
    "        print(f\"Matthew's CC: {metrics['mcc']:.4f}\")\n",
    "        print(f\"Cohen's Kappa: {metrics['kappa']:.4f}\")\n",
    "        \n",
    "        print(\"\\nClinical Metrics:\")\n",
    "        print(f\"Sensitivity/Recall: {metrics['sensitivity']:.4f}\")\n",
    "        print(f\"Specificity: {metrics['specificity']:.4f}\")\n",
    "        print(f\"PPV: {metrics['ppv']:.4f}\")\n",
    "        print(f\"NPV: {metrics['npv']:.4f}\")\n",
    "        print(f\"Positive LR: {metrics['plr']:.2f}\")\n",
    "        print(f\"Negative LR: {metrics['nlr']:.2f}\")\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(\n",
    "            metrics[\"y_true\"],\n",
    "            metrics[\"y_pred\"],\n",
    "            target_names=['Normal', 'Pneumonia']\n",
    "        ))\n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(np.array(metrics[\"confusion_matrix\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = Evaluator()\n",
    "    metrics = evaluator.evaluate()\n",
    "    evaluator.print_results(metrics)\n",
    "    print(f\"\\nEvaluation complete. Results saved to '{evaluator.config['plots_dir']}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ae25e-4d8f-40c9-9437-64bead5a2f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
